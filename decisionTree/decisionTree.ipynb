{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1.1 Implement a decision tree learning algorithm from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    28.7967   16.0021  2.6449  0.3918  0.1982   27.7004   22.011  -8.2027  \\\n0   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238  -9.9574   \n1  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580 -45.2160   \n2   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633  -7.1513   \n3   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525  21.8393   \n4   51.6240   21.1502  2.9085  0.2420  0.1340   50.8761  43.1887   9.8145   \n\n    40.092  81.8828  g  \n0   6.3609  205.261  g  \n1  76.9600  256.788  g  \n2  10.4490  116.737  g  \n3   4.6480  356.462  g  \n4   3.6130  238.098  g  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>28.7967</th>\n      <th>16.0021</th>\n      <th>2.6449</th>\n      <th>0.3918</th>\n      <th>0.1982</th>\n      <th>27.7004</th>\n      <th>22.011</th>\n      <th>-8.2027</th>\n      <th>40.092</th>\n      <th>81.8828</th>\n      <th>g</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>31.6036</td>\n      <td>11.7235</td>\n      <td>2.5185</td>\n      <td>0.5303</td>\n      <td>0.3773</td>\n      <td>26.2722</td>\n      <td>23.8238</td>\n      <td>-9.9574</td>\n      <td>6.3609</td>\n      <td>205.261</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162.0520</td>\n      <td>136.0310</td>\n      <td>4.0612</td>\n      <td>0.0374</td>\n      <td>0.0187</td>\n      <td>116.7410</td>\n      <td>-64.8580</td>\n      <td>-45.2160</td>\n      <td>76.9600</td>\n      <td>256.788</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23.8172</td>\n      <td>9.5728</td>\n      <td>2.3385</td>\n      <td>0.6147</td>\n      <td>0.3922</td>\n      <td>27.2107</td>\n      <td>-6.4633</td>\n      <td>-7.1513</td>\n      <td>10.4490</td>\n      <td>116.737</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.1362</td>\n      <td>30.9205</td>\n      <td>3.1611</td>\n      <td>0.3168</td>\n      <td>0.1832</td>\n      <td>-5.5277</td>\n      <td>28.5525</td>\n      <td>21.8393</td>\n      <td>4.6480</td>\n      <td>356.462</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51.6240</td>\n      <td>21.1502</td>\n      <td>2.9085</td>\n      <td>0.2420</td>\n      <td>0.1340</td>\n      <td>50.8761</td>\n      <td>43.1887</td>\n      <td>9.8145</td>\n      <td>3.6130</td>\n      <td>238.098</td>\n      <td>g</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('magic04.data')\n",
    "X = data.values[:,:-1] \n",
    "Y = data.values[:,-1]\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split the dataset for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 100                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a \n",
    "# concatenation of validation and test sets with a ratio of 0.7/0.3:\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(\n",
    "    X, Y, test_size = 0.3, random_state=seed\n",
    ")\n",
    "\n",
    "# Shuffle and split the data into validation and test sets with a ratio of 0.5/0.5:\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "    X_val_test, Y_val_test, test_size = 0.5, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implement a Decision Tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, index, mean, majority_label):\n",
    "        self.index = index\n",
    "        self.mean = mean\n",
    "        self.majority_label = majority_label\n",
    "\n",
    "    def get_index(self):\n",
    "        return self.index\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"index: %s, mean: %s, ml: %s\" % (self.index, self.mean, self.majority_label)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, label=None, data=None, left=None, right=None):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    deleted_nodes = 0\n",
    "    def __init__(self, dataset=None, label: int = None, root=None): #, branches: List = []):\n",
    "        \"\"\"A Tree has 1 root or *is* the root and has arbitrary many branches\"\"\"\n",
    "        self.impurity_measure = None\n",
    "        self.root = Node()\n",
    "\n",
    "    # split the datasets based on information-gain and entropy or gini\n",
    "    def split(self, dataset, split_index):\n",
    "        split_over, split_under = self.split_over_under(dataset, split_index)\n",
    "\n",
    "        x_left, y_left = split_over.iloc[:,:-1], split_over.iloc[:,-1]\n",
    "        x_right, y_right = split_under.iloc[:,:-1], split_under.iloc[:,-1]\n",
    "\n",
    "        return x_left, y_left, x_right, y_right\n",
    "\n",
    "    # Splits the data into two dataframes, one containing rows where selected column value is above mean, and one below\n",
    "    def split_over_under(self, df, column_index, split_method=\"mean\"):\n",
    "        split_value = 0\n",
    "        if split_method == \"mean\":\n",
    "            split_value = self.get_mean(df, column_index)\n",
    "\n",
    "        # median is not used, but could be used\n",
    "        elif split_method == \"median\":\n",
    "            split_value = df.iloc[:, column_index].median()\n",
    "        else:\n",
    "            raise Exception(f\"Split method {split_method} not recognized.\")\n",
    "\n",
    "        x_split_over = df.loc[df.iloc[:,column_index] >= split_value]\n",
    "        x_split_under = df.loc[df.iloc[:,column_index] < split_value]\n",
    "\n",
    "        return x_split_over, x_split_under\n",
    "\n",
    "    # calculates the entropy\n",
    "    def get_entropy(self, df):\n",
    "        class_labels = df.iloc[:,-1].unique()\n",
    "        entropy = 0\n",
    "\n",
    "        for label in class_labels:\n",
    "            number_of_rows_current_label = len(df.loc[df.iloc[:,-1] == label])\n",
    "            p_current_label = number_of_rows_current_label/len(df)\n",
    "            entropy += -p_current_label * math.log2(p_current_label)\n",
    "        return entropy\n",
    "\n",
    "    # returns the sum of the squared possibility that each of the labels are picked (the gini)\n",
    "    def get_gini(self, df):\n",
    "        class_labels = df.iloc[:,-1].unique()\n",
    "\n",
    "        total_gini = 1\n",
    "        for label in class_labels:\n",
    "            number_of_rows_current_label = len(df.loc[df.iloc[:,-1] == label])\n",
    "            p_current_label = number_of_rows_current_label/len(df)\n",
    "            total_gini -= math.pow(p_current_label, 2)\n",
    "\n",
    "        return total_gini\n",
    "\n",
    "    # calculates the information gain\n",
    "    def get_information_gain(self, df, column_index, split_method=\"mean\"):\n",
    "        split_over, split_under = self.split_over_under(df, column_index, split_method)\n",
    "\n",
    "        if self.impurity_measure == \"gini\":\n",
    "            impurity_split_over = self.get_gini(split_over)\n",
    "            impurity_split_under = self.get_gini(split_under)\n",
    "            prev_impurity_value = self.get_gini(df)\n",
    "        else:\n",
    "            impurity_split_over = self.get_entropy(split_over)\n",
    "            impurity_split_under = self.get_entropy(split_under)\n",
    "            prev_impurity_value = self.get_entropy(df)\n",
    "\n",
    "        information_over = (len(split_over)/len(df)) * impurity_split_over\n",
    "        information_under = (len(split_under)/len(df)) * impurity_split_under\n",
    "        information = information_over + information_under\n",
    "\n",
    "        # return information gain (E(parent) - information(child))\n",
    "        return prev_impurity_value - information\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.predict_tree(x, self.root)\n",
    "\n",
    "    # predicts a label for a given x\n",
    "    def predict_tree(self, x, node):\n",
    "        # if node is a leaf, return the label of the node\n",
    "        if node.is_leaf():\n",
    "            return node.label\n",
    "        # if value at the split index is below split label, traverse down the right side of the tree\n",
    "        if (x[node.data.index] <= node.data.mean).all():\n",
    "            return self.predict_tree(x, node.right)\n",
    "        # continue down the left side\n",
    "        return self.predict_tree(x, node.left)\n",
    "\n",
    "    # builds the tree by creating a node and linking it to a left and a right child (recursively)\n",
    "    def build_tree(self, node, X, y, impurity_measure):\n",
    "        self.impurity_measure = impurity_measure\n",
    "        df = pd.concat([X, y], axis=1)\n",
    "\n",
    "        features, labels = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "        majority_label = self.get_majority_label(labels)\n",
    "        if self.has_same_label(labels):\n",
    "            node.label = majority_label\n",
    "            return\n",
    "        elif self.has_identical_features(features):\n",
    "            node.label = majority_label\n",
    "            return\n",
    "        # get optimal split column index\n",
    "        optimal_split_index = self.get_optimal_split(df)\n",
    "        # LEFT IS ABOVE AND RIGHT IS BELOW\n",
    "        x_left, y_left, x_right, y_right = self.split(df, optimal_split_index)\n",
    "\n",
    "        # get mean used --> save to data\n",
    "        mean = self.get_mean(df, optimal_split_index)\n",
    "        node.label = optimal_split_index\n",
    "        node.data = Data(optimal_split_index, mean, majority_label)\n",
    "\n",
    "        # create new nodes with values split\n",
    "        # build the left side of the tree\n",
    "        if len(y_left) > 0:\n",
    "            node.left = Node()\n",
    "            self.build_tree(node.left, x_left, y_left, impurity_measure)\n",
    "        # build the right side of the tree\n",
    "        if len(y_right) > 0:\n",
    "            node.right = Node()\n",
    "            self.build_tree(node.right, x_right, y_right, impurity_measure)\n",
    "\n",
    "\n",
    "    def get_optimal_split(self, df):\n",
    "        optimal_split_index = 0\n",
    "        # indicates the column with the highest impurity measure based on gini or entropy\n",
    "        current_information_gain = 0\n",
    "        num_columns = df.shape[1]\n",
    "        # get information gain from every possible split and saves the best IG and index\n",
    "        for index in range(num_columns-1):\n",
    "            ig = self.get_information_gain(df, index, \"mean\")\n",
    "\n",
    "            if ig > current_information_gain:\n",
    "                current_information_gain, optimal_split_index = ig, index\n",
    "        return optimal_split_index\n",
    "\n",
    "    def has_same_label(self, y):\n",
    "        # if all labels are equal the length of the set will be 1\n",
    "        if len(set(y)) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def has_identical_features(self, x):\n",
    "        # if all features are equal the length of the set will be 1\n",
    "        if len(set(x)) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def count_all_labels(self, labels):\n",
    "        labels_count = {}\n",
    "        # count the labels and save results in dict\n",
    "        for label in labels:\n",
    "            if label in labels_count:\n",
    "                labels_count[label] += 1\n",
    "            else:\n",
    "                labels_count[label] = 1\n",
    "        return labels_count\n",
    "\n",
    "\n",
    "    def get_majority_label(self, labels):\n",
    "        labels_count = self.count_all_labels(labels)\n",
    "        count = 0\n",
    "        majority_label = \"\"\n",
    "        # get the label with the highest count\n",
    "        for label in labels_count.keys():\n",
    "            label_count = labels_count[label]\n",
    "            if label_count > count:\n",
    "                majority_label = label\n",
    "                count = label_count\n",
    "\n",
    "        return majority_label\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mean(dataset, column_index):\n",
    "        return dataset.iloc[:, column_index].mean()\n",
    "\n",
    "    def learn(self, X_train, y_train, impurity_measure=\"entropy\", prune=False):\n",
    "        # split the data in training and pruning 70/30 if prune is true\n",
    "        if prune:\n",
    "            X_train, X_prune, y_train, y_prune = train_test_split(\n",
    "                X_train, y_train, test_size=0.3, random_state=100\n",
    "            )\n",
    "            X_prune = pd.DataFrame(X_prune)\n",
    "            y_prune = pd.DataFrame(y_prune)\n",
    "\n",
    "        x = pd.DataFrame(X_train)\n",
    "        y = pd.DataFrame(y_train)\n",
    "\n",
    "        self.build_tree(self.root,x, y, impurity_measure)\n",
    "        if prune:\n",
    "            self.prune_tree(X_prune, y_prune.squeeze(), self.root)\n",
    "        return self\n",
    "\n",
    "    def prune_tree(self, X_prune, y_prune, node):\n",
    "        # return the node accuracy if the node is a leaf (has no children)\n",
    "        if node.is_leaf():\n",
    "             return self.count_errors(node.label, y_prune) #self.get_accuracy_score(y_prune, node)\n",
    "\n",
    "        x_left, y_left, x_right, y_right = self.split(pd.concat([X_prune, y_prune], axis=1), node.data.index)\n",
    "\n",
    "        # accuracy score of the children\n",
    "        left_err = self.prune_tree(x_left,y_left, node.left)\n",
    "        right_err = self.prune_tree(x_right, y_right, node.right)\n",
    "\n",
    "        if len(y_left) < 1 or len(y_right) < 1 or len(y_prune) < 1 or left_err is None or right_err is None:\n",
    "            return\n",
    "\n",
    "        parent_err = self.count_errors(node.data.majority_label, y_prune)\n",
    "\n",
    "        if parent_err < left_err + right_err:\n",
    "            self.deleted_nodes += 2\n",
    "            node.label = node.data.majority_label\n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            return parent_err\n",
    "        return left_err + right_err\n",
    "\n",
    "\n",
    "    def count_errors(self, label, y):\n",
    "        count = self.count_all_labels(y)\n",
    "        if label in count:\n",
    "            return len(y) - count[label]\n",
    "        return len(y)\n",
    "\n",
    "\n",
    "def print_tree(node, level=0):\n",
    "    if node != None:\n",
    "        print_tree(node.left, level + 1)\n",
    "        print(' ' * 5 * level + '->', node)\n",
    "        print_tree(node.right, level + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.4 Evaluate Your Algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tree with entropy and prune set to False...\n",
      "Running Tree with entropy and prune set to True...\n",
      "Running Tree with gini and prune set to False...\n",
      "Running Tree with gini and prune set to True...\n",
      "------------------------------------------------------\n",
      "The best setting for validation accuracy is entropy with pruning set to True\n",
      "Validation accuracy = 0.807570977917981\n",
      "------------------------------------------------------\n",
      "Running Tree with entropy and prune set to True...\n",
      "When using the mentioned settings (['entropy', True]) for Test, accuracy = 0.8233438485804416\n"
     ]
    }
   ],
   "source": [
    "def get_pred(X, tree):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(tree.predict(x))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# Run the tree with the specified settings\n",
    "def run_tree(impurity, prune, x, y):\n",
    "    print(f\"Running Tree with {impurity} and prune set to {prune}...\")\n",
    "    tree = Tree().learn(X_train, Y_train, impurity, prune)\n",
    "    acc_score = accuracy_score(y, get_pred(x, tree))\n",
    "    return acc_score\n",
    "\n",
    "# find the settings that gives the best validation accuracy\n",
    "def get_best_val_acc(settings):\n",
    "    val_acc = 0\n",
    "    current_setting = []\n",
    "    for setting in settings:\n",
    "        new_val_acc = run_tree(setting[0], setting[1], X_val, Y_val)\n",
    "        if new_val_acc > val_acc:\n",
    "            val_acc = new_val_acc\n",
    "            current_setting = setting\n",
    "    return current_setting, val_acc\n",
    "\n",
    "tree_setting = [[\"entropy\", False], [\"entropy\", True], [\"gini\", False], [\"gini\", True]]\n",
    "\n",
    "best_setting, val_acc = get_best_val_acc(tree_setting)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(f\"The best setting for validation accuracy is {best_setting[0]} with pruning set to {best_setting[1]}\")\n",
    "print(f\"Validation accuracy = {val_acc}\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(f\"When using the mentioned settings ({best_setting}) for Test, accuracy = {run_tree(best_setting[0], best_setting[1], X_test, Y_test)}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.5 Compare to an existing implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best DecitionTreeClassifier accuricy, was with entropy as impurity, and the score was 0.8314055380301437\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Create implemented DecitionTreeClassifier with entropy parameter\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "clf_val_acc_entropy = accuracy_score(Y_val, clf.predict(X_val))\n",
    "\n",
    "# Create implemented DecitionTreeClassifier with gini parameter\n",
    "clf_gini = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf_gini.fit(X_train, Y_train)\n",
    "\n",
    "clf_val_acc_gini = accuracy_score(Y_val, clf_gini.predict(X_val))\n",
    "\n",
    "if clf_val_acc_gini > clf_val_acc_entropy:\n",
    "    best_test_acc = accuracy_score(Y_test, clf_gini.predict(X_test))\n",
    "    impurity = \"gini\"\n",
    "else:\n",
    "    best_test_acc = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    impurity = \"entropy\"\n",
    "print(f\"The best DecitionTreeClassifier accuricy, was with {impurity} as impurity, and the score was {best_test_acc}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "872bab3c3fe149b9d729a173d2f444bab105618235c94c9facab0fc44f87be5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}